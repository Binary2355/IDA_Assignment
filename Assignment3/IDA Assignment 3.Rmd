---
title: "Assignment 3"
author:
- Zekai Li
- S2040608
header-includes:
- \usepackage{amssymb}
- \usepackage{amsmath}
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies:
    - amssymb
    - amsmath
---

```{r setup, include=FALSE}
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE)
```

```{r library packages, echo=FALSE, results='hide', include=FALSE}
library(purrr)
library(ggplot2)
library(tidyr)
library(dplyr)
library(maxLik)
library(mice)
library(JointAI)
library(devtools)
library(reshape2)
library(RColorBrewer)
library(mitml)
```

## Question 1

### $(a)$ My solution
```{r}
print(c("The propotion of incomplete cases is",
        1-nrow(cc(nhanes))/nrow(nhanes)),quote=FALSE)
```
The percentage of incomplete cases is 48%.

### $(b)$ My solution
```{r}
#nhanes
imps <- mice(nhanes,method="norm",printFlag=FALSE,seed=1)
fits <- with(imps,lm(bmi~age+hyp+chl))
ests <- pool(fits)
print(ests[,3][,c(3,4,5,9,10)])
```
From the pooled estimates, the proportions of variance due to missing data($\frac{B+\frac{B}{M}}{V^\top}$) for intercept is 0.2233797, for the coefficient of "age" is 0.5997594, for the coefficient of "hyp" is 0.4857348, for the coefficient of "chl" is 0.4220811. Looking at the riv, *relative increase in variance*($\frac{B+\frac{B}{M}}{\bar{U}}$). "riv" of the coefficient of "age" is the largest. Therefore, the parameter for "age" appear to be most affected by the nonresponse.

### $(c)$ My solution
```{r}
ests_2 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=2),lm(bmi~age+hyp+chl)))
ests_3 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=3),lm(bmi~age+hyp+chl)))
ests_4 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=4),lm(bmi~age+hyp+chl)))
ests_5 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=5),lm(bmi~age+hyp+chl)))
ests_6 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=6),lm(bmi~age+hyp+chl)))
ests_2[,3][,c(3,4,5,9,10)]
ests_3[,3][,c(3,4,5,9,10)]
ests_4[,3][,c(3,4,5,9,10)]
ests_5[,3][,c(3,4,5,9,10)]
ests_6[,3][,c(3,4,5,9,10)]
```
From the summary of the same model using different seeds, the results do not remain the same.

### $(d)$ My solution
```{r}
ests_m1 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=1,m=100),lm(bmi~age+hyp+chl)))
ests_m2 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=2,m=100),lm(bmi~age+hyp+chl)))
ests_m3 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=3,m=100),lm(bmi~age+hyp+chl)))
ests_m4 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=4,m=100),lm(bmi~age+hyp+chl)))
ests_m5 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=5,m=100),lm(bmi~age+hyp+chl)))
ests_m6 <- pool(with(mice(nhanes,method="norm",printFlag=FALSE,seed=6,m=100),lm(bmi~age+hyp+chl)))
summary(ests_m1,conf.int=TRUE)
summary(ests_m2,conf.int=TRUE)
summary(ests_m3,conf.int=TRUE)
summary(ests_m4,conf.int=TRUE)
summary(ests_m5,conf.int=TRUE)
summary(ests_m6,conf.int=TRUE)
```
The (pooled) estimates, standard errors, and the bounds of the intervals get more stable as M increases and we can be more confident in any one specific run.

## Question 2

### My solution
```{r}
load("dataex2.Rdata")

# built a function to return a matrix with a row 95% confidence intervals for beta_1
# input:  ## D, 2-D, from dataex2
          ## Mtd, a string used in mice() function
# output: ## a matrix
CI_beta1 <- function(D,Mtd){
  imps <- mice(data=D,method=Mtd,printFlag=FALSE,seed=1,m=20)
  ests <- pool(with(imps,lm(Y~X)))
  return(matrix(summary(ests,conf.int=TRUE)[,c(7,8)][2,],nrow=1))
}

# built a function to return a matrix with rows 95% confidence intervals for beta_1
# input:  ## data, 3-D,initialized as dataex2
          ## Mtd, a string used in mice() function
# output: ## a matrix
CIs <- function(data=dataex2,Mtd){
  Conf <- matrix(nrow=1,ncol=2)
  for (i in 1:100){
    A <- CI_beta1(data[,,i],Mtd=Mtd)
    Conf <- rbind(Conf,A)
  }
  return(Conf[2:101,])
}

# function to return a emprical coverage probability
  ## with 95% confidence intervals for beta_1
# input:  ## data, 3-D,initialized as dataex2
          ## Mtd, a string used in mice() function
# output: ## emprical coverage probability(numeric)
ECP <- function(data=dataex2,Mtd){
  n = 100; times = 0
  B = CIs(Mtd=Mtd)
  for (i in 1:100){
    times = ifelse(B[i,][1]<3 & B[i,][2]>3,times+1,times)
  }
  return(times/n)
}

print(c("The emprical probability for ß1 using stochastic regression imputation is",
        ECP(Mtd="norm.nob")),quote=FALSE)
print(c("The emprical probability for ß1 using the corresponding bootstrap based version is",
        ECP(Mtd="norm.boot")),quote=FALSE)
```

## Question 3

### My solution
Assume a linear regression model with M imputations denoted as:
$$
\hat{y}^{(m)}_{mis}=\beta_0+X_{mis}\beta_1+z,\quad z\sim N(0,\hat{\sigma}^2),\,m=1,2,\dots,M
$$
denote $\theta=(\beta_0,\beta_1)$, then:

For case (i), first compute $\hat{\theta}^{(m)}$ for $m=1,2,\dots,M$, then using Rubin's Rules, pool the estimate $\hat{\theta}^{MI}=\frac{1}{M}\sum\limits_{m=1}^M\hat{\theta}^{(m)}$.

For case (ii), 

## Question 4

### $(a)$ My solution
```{r}
load("dataex4.Rdata")
imps <- mice(dataex4,printFlag=FALSE,seed=1,m=50)
ests <- pool(with(imps,lm(y~x1*x2)))
summary(ests,conf.int=TRUE)[,c(1,2,7,8)]
```
The estimates for $\beta_1$ is 1.4112333, and the 95% confident interval is $[1.219397,1.6030697]$;

The estimates for $\beta_2$ is 1.9658191, and the 95% confident interval is $[1.860657,2.0709812]$;

The estimates for $\beta_3$ is 0.7550367, and the 95% confident interval is $[0.642302,0.8677715]$.

### $(b)$ My solution
```{r}
data4 =
  dataex4 %>%
  mutate(inter = x1*x2)

#data4
imps <- mice(data4,printFlag=FALSE,seed=1,m=50)
# change the method using I() method
meth <- imps$method 
meth["inter"] <- "~I(x1*x2)"

# prevent feedback from interaction in the imputation of x1 and x2
pred <- imps$predictorMatrix
# x1*x2 will not be used as predictor of x1 and x2
pred[c("x1", "x2"), "inter"] <- 0
pred[,c("x1","x2")] <- 0
pred["x1","x2"] <- 1
pred["x2","x1"] <- 1

# make sure x1*x2 ordered at last
visSeq <- imps$visitSequence 
which_inter <- match("inter", visSeq)
visSeq <- c(visSeq[-which_inter], visSeq[which_inter])

# passive imputation to impute the interaction variable
imp <- mice(data4,method=meth,predictorMatrix=pred,visitSequence=visSeq,m=50,seed=1,printFlag = FALSE)
ests <- pool(with(imp,lm(y~x1*x2)))
summary(ests,conf.int=TRUE)[,c(1,2,7,8)]
## check problems mice() detected
imp$loggedEvents
```
The estimates for $\beta_1$ is 0.9761881, and the 95% confident interval is $[0.6992222,1.253154]$;

The estimates for $\beta_2$ is 1.6168272, and the 95% confident interval is $[1.4688180,1.764836]$;

The estimates for $\beta_3$ is 0.9470357, and the 95% confident interval is $[0.7999456,1.094126]$.

### $(c)$ My solution
```{r}
imp <- mice(data4,method=meth,m=50,seed=1,printFlag = FALSE)
ests <- pool(with(imp,lm(y~x1+x2+inter)))
summary(ests,conf.int=TRUE)[,c(1,2,7,8)]
```
The estimates for $\beta_1$ is 1.2657606, and the 95% confident interval is $[1.0714517,1.4600696]$;

The estimates for $\beta_2$ is 1.9826229, and the 95% confident interval is $[1.8858124,2.0794333]$;

The estimates for $\beta_3$ is 0.8022453, and the 95% confident interval is $[0.6865434,0.9179472]$.

### $(d)$ My solution
```{r}
imp$predictorMatrix
```
From the Predictor Matrix, we know that it is not modified, this will accompany with a high risk of multi-colinearity.

## Question 5

### My solution 

##### Step1: Inspect

To be started, using the dim() method we see that there are 500 rows, and 12 variables.
```{r}
load('NHANES2.Rdata')
dim(NHANES2)
```
further inspect the nature of our variables and check they are correctly coded.
```{r}
str(NHANES2)
```
Check the simple statistics (min/max/mean/quantiles) of the observed data in each variable along with the number of missing values by summary() method. As we can see, "wgt", "age", and "race" are complete data.
```{r}
summary(NHANES2)
```
Then inspect the missing pattern of the data.(using pacakage JointAI)
```{r}
md_pattern(NHANES2, pattern = FALSE, color = c('#34111b', '#e30f41'))
```
Learnt from the chart showed above, there are 411 observations with observed values on all 12 variables. Also, 29 observations for which bilirubin concentration in mg/dL, High-density lipoprotein cholesterol in mg/dL, and otal serum cholesterol in mg/dL are missing, etc.

Visualise the obeserved data in the missing dataset by pacakage JointAI to see if there is normality between varaibles.
```{r}
par(mar = c(3, 3, 2, 1), mgp = c(2, 0.6, 0)) 
plot_all(NHANES2, breaks = 30, ncol = 4)
```

##### Step1: Imputation

Using the default seeting in the mice() method to see what will happen.
```{r}
imp0 <- mice(NHANES2, maxit = 0) 
imp0
```
From the distribution plot depicting in previous step, It is not unreasonable to change the default imputation method from pmm to norm for the variable "bili", "chol", "HDL", "hgt", "SBP", and "WC".
```{r}
meth <- imp0$method 
meth[c("bili", "chol", "HDL", "hgt", "SBP", "WC")] <- "norm" 
meth
```
For these varibles("bili", "chol", "HDL", "hgt", "SBP", and "WC"), there are risks to impute negative values. 
```{r}
post <- imp0$post
post["bili"] <- "imp[[j]][,i] <- squeeze(imp[[j]][,i], c(0, 20))"
post["chol"] <- "imp[[j]][,i] <- squeeze(imp[[j]][,i], c(0, 100))"
post["HDL"] <- "imp[[j]][,i] <- squeeze(imp[[j]][,i], c(0, 100))"
post["hgt"] <- "imp[[j]][,i] <- squeeze(imp[[j]][,i], c(0, 3))"
post["SBP"] <- "imp[[j]][,i] <- squeeze(imp[[j]][,i], c(0, 300))"
post["WC"] <- "imp[[j]][,i] <- squeeze(imp[[j]][,i], c(0, 200))"
```

Then go for imputation:
```{r}
imp <- mice(NHANES2,method = meth,post=post,maxit=20,m=30,seed=1,printFlag=FALSE)
## check problems mice() detected
imp$loggedEvents
```

Before fitting the model, we have to check the convergence. First we need to visulise the traceplot.
```{r}
plot(imp, layout = c(4,4))
```
from the chart above, we learnt that the iterative algorithm appears to have converged for all variables that were imputed. Then we compare the distribution of the imputed values against the distribution of the observed values. We start doing that for the continuous variables.

```{r}
densityplot(imp)
```
Above, there are 30 red lines which are imputed values, and the the blue ones are from observed data. plots of "chol" and "SBP" perform best. Then check SBP conditional on the gender, and hypertensive status and height conditional on gender.
```{r, fig.width=6,fig.height=3}
densityplot(imp, ~SBP|gender)
densityplot(imp, ~SBP|hypten)
densityplot(imp, ~SBP|gender)
```
From the conditional charts, it seems that differences between the observed and imputed values for SBP affected by gender and hypertensive status, while it seems hard to be seen the relation for the variable height.


```{r, warning=FALSE, results=FALSE, comment=FALSE, cache=FALSE}
source_url("https://gist.githubusercontent.com/NErler/0d00375da460dd33839b98faeee2fdab/raw/c6f537ecf80eddcefd94992ec7926aa57d454536/propplot.R")
```
```{r, warning=FALSE}
propplot(imp)
```
Observed the chart above, there is a large discrepancy between imputed and obeserved data for varible "educ"(educational status), but we do not worry about it for "educ" only has one missing value.

##### Step 3: Fitting values

```{r}
fit <- with(imp, lm(wgt ~ gender + age + hgt + WC))
summary(fit$analyses[[1]])
```

```{r}
comp1 <- complete(imp, 1)
ind <- sample(1:30,1)
plot(fit$analyses[[ind]]$fitted.values, residuals(fit$analyses[[1]]),
     xlab = "Fitted values", ylab = "Residuals")
```
The residual plot shows a little messy, but it cannot be enough to say it is against the homoscedastic assumption.

Seperately compare the response with other variables, we can find that weight do not show some tendency with the increase of age, but there are obvious positive relation with height and waist circumference, which are intuitive. And the box plot illustrates that male heavier than female on average.
```{r}
plot(comp1$wgt ~ comp1$age, xlab = "Age", ylab = "wgt")
plot(comp1$wgt ~ comp1$hgt, xlab = "hgt", ylab = "wgt")
boxplot(comp1$wgt ~ comp1$gender, xlab = "Gender", ylab = "wgt")
plot(comp1$wgt ~ comp1$WC, xlab = "WC", ylab = "wgt")
```
With a qq plot, we do not find anything disobeys our assumption.

```{r}
qqnorm(rstandard(fit$analyses[[1]]), xlim = c(-4, 4), ylim = c(-6, 6))
qqline(rstandard(fit$analyses[[1]]), col = 2)
```

##### Step 4: Pooling out the results

```{r}
pooled_ests <- pool(fit)
summary(pooled_ests, conf.int = TRUE)
```
 
Also, the adjusted $R^2$ is calculated by:
```{r}
pool.r.squared(pooled_ests, adjusted = TRUE)
```

Then we do the Wald test for different variables:
```{r}
fit_no_gender <- with(imp, lm(wgt ~ age + hgt + WC))
fit_no_age <- with(imp, lm(wgt ~ gender + hgt + WC))
fit_no_hgt <- with(imp, lm(wgt ~ gender + age + WC))
fit_no_WC <- with(imp, lm(wgt ~ gender + age + hgt))

# Wald Test for gender
D1(fit, fit_no_gender)
# Wald Test for age
D1(fit, fit_no_age)
# Wald test for hgt
D1(fit, fit_no_hgt)
# Wald test for WC
D1(fit, fit_no_WC)
```

And we can conclude to see that the Wald test statistic of "gender" is not significant, and therefore the gender has no relevant contribution to the SBP model. However, the Wald test statistics of the other three variables are significant, then these three need to be kept in our model. And the conclusion is same as what we conclude at the plot between response and one variable seperately.
 